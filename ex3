import math
import numpy as np
from numpy import *
import scipy.io as sio
from skimage import io
from scipy.optimize import fmin
from copy import deepcopy

file = sio.loadmat('ex3data1.mat')
#print(x)
X = file['X']
y = file['y']


n = np.random.randint(0,5000,54)
ic = []
for i in n:
    #print(i)
    ic.append(np.reshape(X[i,:],(20,20)))

io.imshow_collection(ic)
#io.show()


theta=np.zeros([1,400+1])

k=np.ones([5000,1])
X=append(k,X,1)


def sigmoid(z):
    return 1 / ( 1 + (math.e ** (-z)))


def computeCost(X,y,theta):
    sum=0
    for i in range(len(X)):
        # a következő három sor helyett használj mátrix belső szorzást s = np.dot(X[i,:],theta.transpose())
        s = theta[:,0]
        for j in range(1,len(theta)+1):
            s += theta[:,j]*X[i,j-1]
        #debug-oláshoz mindig használj kiíratást, hogy lásd a változók értékét, most sokat segíthet a print(s)
        #plussz majd valahol arra figyelni, hogy az y[i,0] az egy szám 1-től 10-ig, az osztály címke pedig vagy 1 vagy 0 ehhez a célfüggvény számításhoz
        sum += (-y[i, 0]) * math.log(sigmoid(s)) - (1 - y[i, 0]) * math.log(1 - (sigmoid(s)))
        return sum / len(X)   #ez itt biztosan rossz helyen van, mert már az első ciklus lépés után visszatér


#print(computeCost(X,y,theta))


def gradientDescent(X,y,theta):
    b = np.zeros([5000,1])
    for i in range(len(b)):
        s = theta[:, 0]
        for j in range(1, len(theta) + 1):
            s += theta[:, j] * X[i, j - 1]
        s-=y[i]
        b[i] = s
    theta =  (np.matmul(X.transpose(),b) / len(X)).transpose()
    thetaopt = fmin(computeCost(X,y,theta),theta)
    return thetaopt


print(gradientDescent(X,y,theta))
